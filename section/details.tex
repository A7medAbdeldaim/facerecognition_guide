\section{Implementation Details}
\label{sec:implementation_details}

\subsection{Eigenfaces}
\subsubsection{Performing a PCA in OpenCV}

It's possible to roll your own PCA implementation of Listing \ref{lst:pca} by using \href{http://opencv.willowgarage.com/documentation/cpp/operations_on_arrays.html#cv-eigen}{cv::eigen}, but that's reinventing the wheel. OpenCV2 already comes with \href{http://opencv.willowgarage.com/documentation/cpp/operations_on_arrays.html#pca}{cv::PCA} to do the job. The \href{http://opencv.willowgarage.com/documentation/cpp/operations_on_arrays.html#pca}{cv::PCA} constructor is defined as:

\begin{lstlisting}[language=c++]
cv::PCA::PCA(const Mat& data, const Mat& mean, int flags, int maxComponents=0)
\end{lstlisting}

With the Parameters being:

\begin{itemize}
	\item \textbf{data} The input samples, stored as the matrix rows or as the matrix columns
	\item \textbf{mean} The optional mean value. If the matrix is empty (\lstinline|Mat()|), the mean is computed from the data.
	\item \textbf{flags} Operation flags. Currently the parameter is only used to specify the data layout.
	\begin{itemize}
		\item \textbf{CV\_PCA\_DATA\_AS\_ROWS} Indicates that the input samples are stored as matrix rows.
		\item \textbf{CV\_PCA\_DATA\_AS\_COLS} Indicates that the input samples are stored as matrix columns.
	\end{itemize}
	\item \textbf{maxComponents} The maximum number of components that PCA should retain. By default, all the components are retained.
\end{itemize}

\href{http://opencv.willowgarage.com/documentation/cpp/operations_on_arrays.html#pca}{cv::PCA} expects a \lstinline|Mat| as input, so to perform a PCA on your images you need to put them into a data matrix (similar to Listing \ref{lst:pca}). OpenCV doesn't come with such a function, but it's easy to write a function for turning a vector of Mat into a column matrix (\lstinline|asColumnMatrix|) or row matrix (\lstinline|asRowMatrix|):

\begin{lstlisting}[language=c++]
Mat cv::asColumnMatrix(const vector<Mat>& src, int type) {
  int n = src.size();
  int d = src[0].total();
  Mat data(d, n, type);
  for(int i = 0; i < src.size(); i++) {
    Mat tmp,
      xi = data.col(i);
    src[i].convertTo(tmp, type);
    tmp.reshape(1, d).copyTo(xi);
  }
  return data;
}

Mat cv::asRowMatrix(const vector<Mat>& src, int type) {
  int n = src.size();
  int d = src[0].total();
  Mat data(n, d, type);
  for(int i = 0; i < src.size(); i++) {
    Mat tmp,
      xi = data.row(i);
    src[i].convertTo(tmp, type);
    tmp.reshape(1, 1).copyTo(xi);
  }
  return data;
}
\end{lstlisting}

The only difference to the MATLAB implementation is, that OpenCV2 stores the eigenvectors by row. Simply transpose the eigenvectors of the \href{http://opencv.willowgarage.com/documentation/cpp/operations_on_arrays.html#pca}{cv::PCA} to be in sync with the MATLAB implementation (makes your life easier).

\subsubsection{Projection and Reconstruction}

To implement Equation \ref{eqn:pca_projection} in OpenCV you first need to subtract the mean from given data. This can be done with \href{http://opencv.willowgarage.com/documentation/cpp/core_operations_on_arrays.html#cv-subtract}{cv::subtract}, note that I've used \href{http://opencv.willowgarage.com/documentation/cpp/core_operations_on_arrays.html#cv-repeat}{cv::repeat} to repeat the mean vector for each sample. You can achieve the same by iterating over the data, which is probably better for large datasets.

\begin{lstlisting}[language=c++,caption={\lstinline|project| projects a given image into the PCA subspace. \label{lst:opencv_projection}}]
Mat project(const Mat& W, const Mat& mean, const Mat& src, bool dataAsRow) {
  Mat X,Y;
  int n = dataAsRow ? src.rows : src.cols;
  subtract(dataAsRow ? src : transpose(src),
      repeat(dataAsRow ? mean : transpose(mean), n, 1),
      X,
      Mat(),
      W.type());
  gemm(X, W, 1.0, Mat(), 0.0, Y);
  return dataAsRow ? Y : transpose(Y);
}
\end{lstlisting}

Implementing Equation \ref{eqn:pca_reconstruction} works just the same, except one call to \href{http://opencv.willowgarage.com/documentation/cpp/core_operations_on_arrays.html#cv-gemm}{cv::gemm} already does the job. I've used \href{http://opencv.willowgarage.com/documentation/cpp/core_operations_on_arrays.html#cv-repeat}{cv::repeat} again, add it by iterating over your data instead (if you count the bytes).

\begin{lstlisting}[language=c++, caption={\lstinline|reconstruct| reconstructs a given coefficient vector from the PCA basis. \label{lst:opencv_reconstruction}}]
Mat reconstruct(const Mat& W, const Mat& mean, const Mat& src, bool dataAsRow) {
  Mat X;
  int n = dataAsRow ? src.rows : src.cols;
  gemm(dataAsRow ? src : transpose(src),
      W,
      1.0,
      repeat(dataAsRow ? mean : transpose(mean), n, 1),
      1.0,
      X,
      GEMM_2_T);
  return dataAsRow ? X : transpose(X);
}
\end{lstlisting}

\subsubsection{Prediction}

Listing \ref{lst:opencv_prediction} shows how predictions are generated within the Eigenfaces class. I didn't introduce a threshold as proposed in \cite{PT91}, because it would even further confuse people. If your use case needs a threshold, then this is the point where to add it. Quick review of the Eigenfaces method: It makes a prediction by 

\begin{enumerate}
	\item Projecting all training samples into the PCA subspace (using Listing \ref{lst:opencv_projection}).
	\item Projecting the query image into the PCA subspace (using Listing \ref{lst:opencv_reconstruction}).
	\item Finding the nearest neighbor between the projected training images and projected query. This implementation uses the euclidean distance as distance metric. If you want to experiment with other metrics, this is the point where you have to add it.
\end{enumerate}

\begin{lstlisting}[language=c++, caption={\lstinline|Eigenfaces::predict| gets a prediction for a given query image. \label{lst:opencv_prediction}}]
int Eigenfaces::predict(const Mat& src) {
  Mat q = project(_dataAsRow ? src.reshape(1,1) : src.reshape(1, src.total()));
  // find 1-nearest neighbor
  double minDist = numeric_limits<double>::max();
  int minClass = -1;
  for(int sampleIdx = 0; sampleIdx < _projections.size(); sampleIdx++) {
    double dist = norm(_projections[sampleIdx], q, NORM_L2);
    if(dist < minDist) {
      minDist = dist;
      minClass = _labels[sampleIdx];
    }
  }
  return minClass;
}
\end{lstlisting}

\subsection{Linear Discriminant Analysis}




